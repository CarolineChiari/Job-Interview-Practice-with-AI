{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Interview Wihth OpenAI's GPT model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you need to get started\n",
    "\n",
    "This notebook needs a `.env` file in the same folder with the following data:\n",
    "```\n",
    "SPEECH_KEY=<Cognitive services key>\n",
    "SPEECH_REGION=<cognitive service region>\n",
    "SPEECH_LANGUAGE=<cognitive services language>\n",
    "OPENAI_KEY=<openAI API key>\n",
    "```\n",
    "\n",
    "## How do I get the speech key and region?\n",
    "\n",
    "You need a valid [Azure](https://portal.azure.com/) subscription in order to get those keys.\n",
    "\n",
    "Once you do, follow these steps:\n",
    "- (optional) Create a new Azure Resource Group and call it `speech`\n",
    "- From the resource group, click on **Create** and search for `Cognitive  Services`\n",
    "- Select the first one anc click **Create**\n",
    "- Choose your subscription and resource group. Also provide a name and choose the pricing tier. You can find more information about Cognitive services' pricing [here](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/speech-services/)\n",
    "- Leave everything else as default and click **Review + Create** then click **Create** if there are no issues.\n",
    "- Once the service is created, either click on **Go to Resource** or navigate to your resource\n",
    "- Click on **Keys and Endpoint** on the left pane\n",
    "- From there you can copy your key and region, and paste it in the `.env` file mentioned above.\n",
    "\n",
    "## How do I get the OpenAI Key?\n",
    "\n",
    "You need a valid paid subscription to OpenAI. More information about costs [here](https://openai.com/api/pricing/)\n",
    "\n",
    "- Go to [OpenAI's API website](https://openai.com/api/)\n",
    "- **Log In** or **Sign Up** if you haven't done so already\n",
    "- Click on your profile in the right hand side of the screen\n",
    "- Click on **View API Keys**\n",
    "- Click on **Create New Secret Key**\n",
    "- Copy and paste the value in the `.env` file mentioned above.\n",
    "\n",
    "## How to use this notebook?\n",
    "\n",
    "In the code box below, replace the `jobTitle` with the job title you want to practice with.\n",
    "Enter the `numberOfQuestions` you want to practice on.\n",
    "\n",
    "Run all the cells.\n",
    "\n",
    "When the AI asks you a question, there will be a shot tone to indicate that it's ready to listen to you. If you stop talking for 2.5s, it will automatically assume you are done answering and move to the next question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the interview parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobTitle = \"Cloud Engineer\"\n",
    "numberOfQuestions = 5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install azure-cognitiveservices-speech\n",
    "! pip3 install python-dotenv\n",
    "! pip3 install openai\n",
    "! pip3 install simpleaudio\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "settings = {\n",
    "    'speechKey': os.environ.get('SPEECH_KEY'),\n",
    "    'region': os.environ.get('SPEECH_REGION'),\n",
    "    # Feel free to hardcode the language\n",
    "    'language': os.environ.get('SPEECH_LANGUAGE'),\n",
    "    'openAIKey': os.environ.get('OPENAI_KEY')\n",
    "}\n",
    "# Load your API key from an environment variable or secret management service\n",
    "openai.api_key = settings['openAIKey']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_openai(prompt, token=50):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.9,\n",
    "        max_tokens=token,\n",
    "        top_p=1\n",
    "    )\n",
    "    lines = response.to_dict_recursive()['choices'][0]['text'].split(\"\\n\")\n",
    "    response = list(filter(lambda x: x != '', lines))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a sound to indicate recording session is on.\n",
    "import simpleaudio as sa\n",
    "import numpy as np\n",
    "\n",
    "def play_sound():\n",
    "    # set the frequency and duration\n",
    "    frequency = 440\n",
    "    duration = 0.1  # in seconds\n",
    "\n",
    "    # create a waveform\n",
    "    sample_rate = 44100\n",
    "    amplitude = 16000\n",
    "\n",
    "    waveform = np.sin(2 * np.pi * np.arange(sample_rate * duration) * frequency / sample_rate)\n",
    "    waveform = (waveform * amplitude).astype(np.int16)\n",
    "\n",
    "    # create a simpleaudio object\n",
    "    audio = sa.play_buffer(waveform, 1, 2, sample_rate)\n",
    "\n",
    "    # wait for the waveform to finish playing\n",
    "    audio.wait_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time\n",
    "\n",
    "prop = False\n",
    "\n",
    "\n",
    "def Start_recording_answer():\n",
    "    # print(\"===================================\")\n",
    "    # print(\"Processing \"+settings['fileName'])\n",
    "    # print(\"===================================\")\n",
    "\n",
    "    # Creates an instance of a speech config with specified subscription key and service region.\n",
    "    speech_config = speechsdk.SpeechConfig(\n",
    "        subscription=settings['speechKey'], region=settings['region'])\n",
    "\n",
    "    speech_config.request_word_level_timestamps()\n",
    "    speech_config.set_property(\n",
    "        property_id=speechsdk.PropertyId.SpeechServiceResponse_OutputFormatOption, value=\"detailed\")\n",
    "\n",
    "    # Creates a speech recognizer using file as audio input.\n",
    "    # device_name=\"BuiltInMicrophoneDevice\")\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "\n",
    "    # audio_input = speechsdk.audio.AudioConfig(filename=settings['fileName'])\n",
    "\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "    #     speech_config=speech_config, language=settings['language'], audio_config=audio_input)\n",
    "    # translator = speechsdk.translation.TranslationRecognizer(\n",
    "    #     translation_config=translation_config, audio_config=audio_input)\n",
    "\n",
    "    # initialize some variables\n",
    "    results = []\n",
    "    done = False\n",
    "    \n",
    "    def speech_detected():\n",
    "        nonlocal lastSpoken\n",
    "        lastSpoken = int(datetime.now().timestamp() * 1000)\n",
    "\n",
    "    # Event handler to add event to the result list\n",
    "    def handleResult(evt):\n",
    "        import json\n",
    "        nonlocal results\n",
    "        nonlocal lastSpoken\n",
    "        results.append(json.loads(evt.result.json))\n",
    "\n",
    "        # print the result (optional, otherwise it can run for a few minutes without output)\n",
    "        # print('RECOGNIZED: {}'.format(evt))\n",
    "        speech_detected()\n",
    "\n",
    "        # result object\n",
    "        res = {'text': evt.result.test, 'timestamp': evt.result.offset,\n",
    "               'duration': evt.result.duration, 'raw': evt.result}\n",
    "\n",
    "        if (evt.result.text != \"\"):\n",
    "            results.append(res)\n",
    "            \n",
    "            # print(evt.result)\n",
    "\n",
    "    # Event handler to check if the recognizer is done\n",
    "\n",
    "    def stop_cb(evt):\n",
    "        # print('CLOSING on {}'.format(evt))\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    # Connect callbacks to the events fired by the speech recognizer & displays the info/status\n",
    "    # Ref:https://docs.microsoft.com/en-us/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.eventsignal?view=azure-python\n",
    "    speech_recognizer.recognizing.connect(lambda evt: speech_detected())\n",
    "    # speech_recognizer.recognized.connect(lambda evt: print('RECOGNIZED: {}'.format(evt)))\n",
    "    speech_recognizer.session_started.connect(\n",
    "        lambda evt: print('SESSION STARTED: {}'.format(evt)))\n",
    "    speech_recognizer.session_stopped.connect(\n",
    "        lambda evt: print('SESSION STOPPED {}'.format(evt)))\n",
    "    speech_recognizer.canceled.connect(\n",
    "        lambda evt: print('CANCELED {}'.format(evt)))\n",
    "    speech_recognizer.recognized.connect(handleResult)\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Starts continuous speech recognition\n",
    "    # speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "    # Starts continuous speech recognition\n",
    "    # translator.start_continuous_recognition()\n",
    "    \n",
    "    result_future = speech_recognizer.start_continuous_recognition_async()\n",
    "    result_future.get()\n",
    "    play_sound()\n",
    "    lastSpoken = int(datetime.now().timestamp() * 1000)\n",
    "    # Wait for speech recognition to complete\n",
    "    while not done:\n",
    "        time.sleep(1)\n",
    "        now = int(datetime.now().timestamp() * 1000)\n",
    "        inactivity = now - lastSpoken\n",
    "        # print(inactivity)\n",
    "        if (inactivity > 2500):\n",
    "            print('Stopping async recognition.')\n",
    "            speech_recognizer.stop_continuous_recognition_async()\n",
    "            speak(\"Thank you!\")\n",
    "            while not done:\n",
    "                time.sleep(1)\n",
    "\n",
    "    output = \"\"\n",
    "    for res in results:\n",
    "        output += res['NBest'][0]['Display']\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "\n",
    "def speak(text):\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_config = speechsdk.SpeechConfig(\n",
    "        subscription=settings['speechKey'], region=settings['region'])\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "    # The language of the voice that speaks.\n",
    "    speech_config.speech_synthesis_voice_name = 'en-US-JennyNeural'\n",
    "\n",
    "    speech_synthesizer = speechsdk.SpeechSynthesizer(\n",
    "        speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    speech_synthesis_result = speech_synthesizer.speak_text(text) #.get()\n",
    "\n",
    "    if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(\"Speech synthesized for text [{}]\".format(text))\n",
    "    elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_synthesis_result.cancellation_details\n",
    "        print(\"Speech synthesis canceled: {}\".format(\n",
    "            cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            if cancellation_details.error_details:\n",
    "                print(\"Error details: {}\".format(\n",
    "                    cancellation_details.error_details))\n",
    "                print(\"Did you set the speech resource key and region values?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feedback(interviewer, interviewee):\n",
    "    prompt = \"\"\"\n",
    "write a paragraph to the interviewee describing what they did well in their response, and how they can improve:\n",
    "\n",
    "interviewer: {0}\n",
    "\n",
    "interviewee: {1}\n",
    "\"\"\".format(interviewer, interviewee)\n",
    "    print(prompt)\n",
    "    res = complete_openai(prompt=prompt, token=int(4000-len(prompt.split())*1.75))\n",
    "    res = (\"\\n\".join(res))\n",
    "    return {'feedback':res,'score':get_score(interviewer=interviewer,interviewee=interviewee)}\n",
    "\n",
    "def get_score(interviewer, interviewee):\n",
    "    prompt = \"\"\"\n",
    "on a scale from 1 to 10 where 1 is the worst performance and 10 is the best, rater the interviewee's answer (only give me a number, no words):\n",
    "\n",
    "interviewer: {0}\n",
    "\n",
    "interviewee: {1}\n",
    "\"\"\".format(interviewer, interviewee)\n",
    "    print(prompt)\n",
    "    res = complete_openai(prompt=prompt, token=int(4000-len(prompt.split())*1.75))\n",
    "    res = (\"\\n\".join(res))\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = complete_openai(\n",
    "    prompt=\"Write the very first dialog for an interviewer interviewing an interviewee applying for a {0} job. Don't write any of the interviewee parts, and only write 1 part for the interviewer\".format(jobTitle), token=3000)\n",
    "introDialog = (\"\\n\".join(intro)).replace(\"Interviewer: \", \"\")\n",
    "speak(introDialog)\n",
    "answer = Start_recording_answer()\n",
    "feedback = get_feedback(introDialog, answer[0]['DisplayText'])\n",
    "dialog += [{\n",
    "    'interviewer': introDialog,\n",
    "    'interviewee': answer[0]['DisplayText'],\n",
    "    'feedback': feedback['feedback'],\n",
    "    'score': feedback['score']\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, numberOfQuestions):\n",
    "    prompt = \"\"\"\n",
    "        interviewer: {0}\n",
    "\n",
    "        interviewee: {1}\n",
    "\n",
    "        interviewer:\n",
    "        \"\"\".format(dialog[len(dialog)-1]['interviewer'], dialog[len(dialog)-1]['interviewee'])\n",
    "    interviewer = complete_openai(\n",
    "        prompt=prompt,\n",
    "        token=600\n",
    "    )\n",
    "    print(interviewer[0])\n",
    "    print(prompt)\n",
    "    speak(interviewer[0])\n",
    "    answer = Start_recording_answer()\n",
    "    feedback = get_feedback(interviewer[0], answer[0]['DisplayText'])\n",
    "    dialog += [{\n",
    "        'interviewer': interviewer[0],\n",
    "        'interviewee': answer[0]['DisplayText'],\n",
    "        'feedback': feedback['feedback'],\n",
    "        'score': feedback['score']\n",
    "    }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speak(\"Here is your feedback:\")\n",
    "for interaction in dialog:\n",
    "    speak(\"\"\"\n",
    "for the question: {0}\n",
    "\n",
    "the feedback is: {1}\n",
    "\"\"\".format(interaction['interviewer'],interaction['feedback']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
