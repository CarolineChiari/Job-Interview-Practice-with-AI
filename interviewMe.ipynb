{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Interview Wihth OpenAI's GPT model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the interview parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobTitle = \"Cloud Engineer\"\n",
    "numberOfQuestions = 5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install - -upgrade pip\n",
    "! pip3 install azure-cognitiveservices-speech\n",
    "! pip3 install python-dotenv\n",
    "! pip3 install openai\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "settings = {\n",
    "    'speechKey': os.environ.get('SPEECH_KEY'),\n",
    "    'region': os.environ.get('SPEECH_REGION'),\n",
    "    # Feel free to hardcode the language\n",
    "    'language': os.environ.get('SPEECH_LANGUAGE'),\n",
    "    'openAIKey': os.environ.get('OPENAI_KEY')\n",
    "}\n",
    "# Load your API key from an environment variable or secret management service\n",
    "openai.api_key = settings['openAIKey']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_openai(prompt, token=50):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.9,\n",
    "        max_tokens=token,\n",
    "        top_p=1\n",
    "    )\n",
    "    lines = response.to_dict_recursive()['choices'][0]['text'].split(\"\\n\")\n",
    "    response = list(filter(lambda x: x != '', lines))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time\n",
    "\n",
    "prop = False\n",
    "\n",
    "\n",
    "def Start_recording_answer():\n",
    "    # print(\"===================================\")\n",
    "    # print(\"Processing \"+settings['fileName'])\n",
    "    # print(\"===================================\")\n",
    "\n",
    "    # Creates an instance of a speech config with specified subscription key and service region.\n",
    "    speech_config = speechsdk.SpeechConfig(\n",
    "        subscription=settings['speechKey'], region=settings['region'])\n",
    "\n",
    "    speech_config.request_word_level_timestamps()\n",
    "    speech_config.set_property(\n",
    "        property_id=speechsdk.PropertyId.SpeechServiceResponse_OutputFormatOption, value=\"detailed\")\n",
    "\n",
    "    # Creates a speech recognizer using file as audio input.\n",
    "    # device_name=\"BuiltInMicrophoneDevice\")\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "\n",
    "    # audio_input = speechsdk.audio.AudioConfig(filename=settings['fileName'])\n",
    "\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "        speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    # speech_recognizer = speechsdk.SpeechRecognizer(\n",
    "    #     speech_config=speech_config, language=settings['language'], audio_config=audio_input)\n",
    "    # translator = speechsdk.translation.TranslationRecognizer(\n",
    "    #     translation_config=translation_config, audio_config=audio_input)\n",
    "\n",
    "    # initialize some variables\n",
    "    results = []\n",
    "    done = False\n",
    "\n",
    "    # Event handler to add event to the result list\n",
    "\n",
    "    def handleResult(evt):\n",
    "        import json\n",
    "        nonlocal results\n",
    "        results.append(json.loads(evt.result.json))\n",
    "\n",
    "        # print the result (optional, otherwise it can run for a few minutes without output)\n",
    "        # print('RECOGNIZED: {}'.format(evt))\n",
    "\n",
    "        # result object\n",
    "        res = {'text': evt.result.test, 'timestamp': evt.result.offset,\n",
    "               'duration': evt.result.duration, 'raw': evt.result}\n",
    "\n",
    "        if (evt.result.text != \"\"):\n",
    "            results.append(res)\n",
    "            # print(evt.result)\n",
    "\n",
    "    # Event handler to check if the recognizer is done\n",
    "\n",
    "    def stop_cb(evt):\n",
    "        # print('CLOSING on {}'.format(evt))\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    # Connect callbacks to the events fired by the speech recognizer & displays the info/status\n",
    "    # Ref:https://docs.microsoft.com/en-us/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech.eventsignal?view=azure-python\n",
    "    # speech_recognizer.recognizing.connect(lambda evt: print('RECOGNIZING: {}'.format(evt)))\n",
    "    # speech_recognizer.recognized.connect(lambda evt: print('RECOGNIZED: {}'.format(evt)))\n",
    "    speech_recognizer.session_started.connect(\n",
    "        lambda evt: print('SESSION STARTED: {}'.format(evt)))\n",
    "    speech_recognizer.session_stopped.connect(\n",
    "        lambda evt: print('SESSION STOPPED {}'.format(evt)))\n",
    "    speech_recognizer.canceled.connect(\n",
    "        lambda evt: print('CANCELED {}'.format(evt)))\n",
    "    speech_recognizer.recognized.connect(handleResult)\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Starts continuous speech recognition\n",
    "    # speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "    # Starts continuous speech recognition\n",
    "    # translator.start_continuous_recognition()\n",
    "\n",
    "    result_future = speech_recognizer.start_continuous_recognition_async()\n",
    "    result_future.get()\n",
    "    # Wait for speech recognition to complete\n",
    "    while not done:\n",
    "        time.sleep(1)\n",
    "        print('type \"stop\" then enter when done')\n",
    "        stop = input()\n",
    "        if (stop.lower() == \"stop\"):\n",
    "            print('Stopping async recognition.')\n",
    "            speech_recognizer.stop_continuous_recognition_async()\n",
    "            while not done:\n",
    "                time.sleep(1)\n",
    "\n",
    "    output = \"\"\n",
    "    for res in results:\n",
    "        output += res['NBest'][0]['Display']\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "\n",
    "def speak(text):\n",
    "    # This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "    speech_config = speechsdk.SpeechConfig(\n",
    "        subscription=settings['speechKey'], region=settings['region'])\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "    # The language of the voice that speaks.\n",
    "    speech_config.speech_synthesis_voice_name = 'en-US-JennyNeural'\n",
    "\n",
    "    speech_synthesizer = speechsdk.SpeechSynthesizer(\n",
    "        speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
    "\n",
    "    if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(\"Speech synthesized for text [{}]\".format(text))\n",
    "    elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_synthesis_result.cancellation_details\n",
    "        print(\"Speech synthesis canceled: {}\".format(\n",
    "            cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            if cancellation_details.error_details:\n",
    "                print(\"Error details: {}\".format(\n",
    "                    cancellation_details.error_details))\n",
    "                print(\"Did you set the speech resource key and region values?\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = []\n",
    "\n",
    "\n",
    "def get_feedback(interviewer, interviewee):\n",
    "    prompt = \"\"\"\n",
    "write a paragraph to the interviewee describing what they did well in their response, and how they can improve:\n",
    "\n",
    "interviewer: {0}\n",
    "\n",
    "interviewee: {1}\n",
    "\"\"\".format(interviewer, interviewee)\n",
    "    print(prompt)\n",
    "    res = complete_openai(prompt=prompt, token=int(4000-len(prompt.split())*1.75))\n",
    "    res = (\"\\n\".join(res))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = complete_openai(\n",
    "    prompt=\"Write the very first dialog for an interviewer interviewing an interviewee applying for a {0} job. Don't write any of the interviewee parts, and only write 1 part for the interviewer\".format(jobTitle), token=3000)\n",
    "introDialog = (\"\\n\".join(intro)).replace(\"Interviewer: \", \"\")\n",
    "speak(introDialog)\n",
    "answer = Start_recording_answer()\n",
    "feedback = get_feedback(introDialog, answer[0]['DisplayText'])\n",
    "dialog += [{\n",
    "    'interviewer': introDialog,\n",
    "    'interviewee': answer[0]['DisplayText'],\n",
    "    'feedback': feedback\n",
    "}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, numberOfQuestions):\n",
    "    prompt = \"\"\"\n",
    "        interviewer: {0}\n",
    "\n",
    "        interviewee: {1}\n",
    "\n",
    "        interviewer:\n",
    "        \"\"\".format(dialog[len(dialog)-1]['interviewer'], dialog[len(dialog)-1]['interviewee'])\n",
    "    interviewer = complete_openai(\n",
    "        prompt=prompt,\n",
    "        token=600\n",
    "    )\n",
    "    print(interviewer[0])\n",
    "    print(prompt)\n",
    "    speak(interviewer[0])\n",
    "    answer = Start_recording_answer()\n",
    "    feedback = get_feedback(interviewer[0], answer[0]['DisplayText'])\n",
    "    dialog += [{\n",
    "        'interviewer': introDialog,\n",
    "        'interviewee': answer[0]['DisplayText'],\n",
    "        'feedback': feedback\n",
    "    }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for interaction in dialog:\n",
    "    speak(\"\"\"\n",
    "for the question: {0}\n",
    "\n",
    "the feedback is: {1}\n",
    "\"\"\".format(interaction['interviewer'],interaction['feedback']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
